{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igscBfg31xzX"
      },
      "source": [
        "## Natural language processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vzooI-nvkzO"
      },
      "source": [
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Go7rqn6FIvn",
        "outputId": "751f29c4-4e4d-4990-cf6f-0c9b1eea50a8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbuU8WhzKLBt"
      },
      "source": [
        "# take the source text for analysis\n",
        "corpus = 'When we were in Paris we visited a lot of museums. We first went to the Louvre, the largest art museum in the world. I have always been interested in art so I spent many hours there. The museum is enourmous, so a week there would not be enough.'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZCOyIopqhJJ"
      },
      "source": [
        "# importing the main library for working with text\n",
        "import nltk\n",
        "\n",
        "# and other libraries already known to us\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiXzm8bSWuR8"
      },
      "source": [
        "#### Step 1. Splitting into sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "0_lp7a2jqXKV",
        "outputId": "c5956343-1251-44ba-e6a6-f28fc927be86"
      },
      "source": [
        "# importing the sent_tokenize() function\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# download the model that will divide the text into sentences\n",
        "nltk.download('punkt')\n",
        "print('')\n",
        "\n",
        "# and apply the function to the text\n",
        "sentences = sent_tokenize(corpus)\n",
        "sentences"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5dd8dd7c9306>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# and apply the function to the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yQB6XfgW5J7"
      },
      "source": [
        "#### Step 2. Splitting into words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYo3qgIGtSNy"
      },
      "source": [
        "# importing the word_tokenize() function\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# and let's break down the first sentence into words\n",
        "print(word_tokenize(sentences[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1QCDw-KtXb1"
      },
      "source": [
        "# now let's do this with all the offers\n",
        "\n",
        "# to do this, create an empty list\n",
        "tokens = []\n",
        "\n",
        "# in the for loop, we'll go through each sentence\n",
        "for sentence in sentences:\n",
        "\n",
        "    # creating lists of tokens\n",
        "    t = word_tokenize(sentence)\n",
        "\n",
        "    # and add the lists to each other\n",
        "    tokens.extend(t)\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZlVHeXfW-EV"
      },
      "source": [
        "#### Step 3. Lowercase translation, removal of stop words and punctuation marks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86qHziOHtcwh"
      },
      "source": [
        "# importing the stopword module\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# download the dictionary of stop words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# we use set to leave only unique values\n",
        "unique_stops = set(stopwords.words('english'))\n",
        "\n",
        "# creating an empty list without stop words\n",
        "no_stops = []\n",
        "\n",
        "# going through all the tokens\n",
        "for token in tokens:\n",
        "\n",
        "    # we translate all words into lowercase\n",
        "    token = token.lower()\n",
        "\n",
        "    # если тоif the token is not in the list of stop words and is not a punctuation mark,\n",
        "    if token not in unique_stops and token.isalpha():\n",
        "\n",
        "        # adding it to the list\n",
        "        no_stops.append(token)\n",
        "\n",
        "print(no_stops)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4XqH_LbXKNI"
      },
      "source": [
        "#### Step 4. Lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_QyS1s6wlxb"
      },
      "source": [
        "# importing a class for lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# importing the dictionary\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# creating an object of this class\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# and an empty list for words after lemmatization\n",
        "lemmatized = []\n",
        "\n",
        "# going through all the tokens\n",
        "for token in no_stops:\n",
        "\n",
        "    # we apply lemmatization\n",
        "    token = lemmatizer.lemmatize(token)\n",
        "\n",
        "    # adding the word after lemmatization to the list\n",
        "    lemmatized.append(token)\n",
        "\n",
        "print(lemmatized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq7a3URaXM_i"
      },
      "source": [
        "#### Step 5. Stemming\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69xOHuD80wis"
      },
      "source": [
        "# importing the Porter stemmer class and creating an object of this class\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "# we use list comprehension instead of the for loop for stemming and creating a new list\n",
        "# this record is much shorter.\n",
        "stemmed_p = [porter.stem(s) for s in lemmatized]\n",
        "print(stemmed_p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eAgmFso2NCL"
      },
      "source": [
        "# similarly, we import the Lancaster class and create an object of this class\n",
        "from nltk.stem import LancasterStemmer\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "# We also use list_comprehension\n",
        "stemmed_l = [lancaster.stem(s) for s in lemmatized]\n",
        "print(stemmed_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg_PhGXGvhJG"
      },
      "source": [
        "### Bag of words (bag of words, bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyw-EPbs2mV_"
      },
      "source": [
        "Using Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqpe0QUEvfbp"
      },
      "source": [
        "# importing the Counter class from the collections module\n",
        "from collections import Counter\n",
        "\n",
        "# applying the Counter class to words after lemmatization\n",
        "# the output returns the dictionary { word : its frequency in the text }\n",
        "bow_counter = Counter(lemmatized)\n",
        "# print(bow_counter)\n",
        "\n",
        "\n",
        "# the most_common() function orders the dictionary by value\n",
        "# look at the top 10 most frequent words\n",
        "print(bow_counter.most_common(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxDQoiXi2qUV"
      },
      "source": [
        "Using CountVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLKaxvgc2_hr"
      },
      "source": [
        "# importing the CountVectorizer class from the Scikit-learn library\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# creating an object of this class and\n",
        "# we indicate that we want to translate the words into lowercase, as well as\n",
        "# filter out stop words using stop_words = 'english'\n",
        "vectorizer = CountVectorizer(analyzer = \"word\",\n",
        "                             lowercase = True,\n",
        "                             tokenizer = None,\n",
        "                             preprocessor = None,\n",
        "                             stop_words = 'english',\n",
        "                             max_features = 5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG1oEHVK44nA"
      },
      "source": [
        "# we apply this object to sentences (they also say documents)\n",
        "bow_cv = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# the output is a csr matrix\n",
        "print(type(bow_cv))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rv2IbfGRWIU"
      },
      "source": [
        "# convert the csr matrix to the familiar Numpy array format\n",
        "# to do this, you can use .toarray()\n",
        "print(bow_cv.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC5XjZr8b70w"
      },
      "source": [
        "# rows are sentences (documents), columns are words (tokens)\n",
        "bow_cv.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDztE1R_4-1h"
      },
      "source": [
        "# we can look at the tokens (words) used\n",
        "\n",
        "# Here, numbers are not a frequency, but simply an ordinal number (index).\n",
        "vocab = vectorizer.vocabulary_\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can output words without an index\n",
        "tokens = vectorizer.get_feature_names_out()\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "If6Vb-X4skzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaNy4TEucTse"
      },
      "source": [
        "# for convenience, we will convert the matrix into a dataframe\n",
        "\n",
        "# first, let's create an index of proposals (documents)\n",
        "index_list = []\n",
        "\n",
        "# in the loop, we will go through the elements of the matrix, denoting them by '_'\n",
        "# the enumerate function will set an index for each element, starting from 0\n",
        "for i, _ in enumerate(bow_cv):\n",
        "\n",
        "    # let's add an index to the word Sentence\n",
        "    index_list.append(f'Sentence_{i}')\n",
        "\n",
        "# print(index_list)\n",
        "\n",
        "# now you can use pd.DataFrame()\n",
        "bow_cv_df = pd.DataFrame(data = bow_cv.toarray(),\n",
        "                         index = index_list,\n",
        "                         columns = tokens)\n",
        "bow_cv_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahoLL6IhwVRH"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVomqbuze8nz"
      },
      "source": [
        "#### Method 1. CountVectorizer + TfidfTransformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "023pB7-taywP"
      },
      "source": [
        "1) Calculation of TF, term frequency, word frequency\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRaJjVB3f0By"
      },
      "source": [
        "# We have already completed this step above\n",
        "bow_cv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtVVXeMYbBMJ"
      },
      "source": [
        "2) Now you need to calculate the IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZd-UCdufc-Z"
      },
      "source": [
        "# importing TfidfTransformer (CountVectorizer has already been imported)\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# creating an object of the Tf idf Transformer class\n",
        "tfidf_trans = TfidfTransformer(smooth_idf = True, use_idf = True)\n",
        "\n",
        "# and calculate the IDF of words\n",
        "tfidf_trans.fit(bow_cv)\n",
        "\n",
        "# putting the result in a dataframe\n",
        "df_idf = pd.DataFrame(tfidf_trans.idf_, index = tokens, columns = [\"idf_weights\"])\n",
        "#df_idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TTWk4ZxbPHB"
      },
      "source": [
        "3) That leaves TFxIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSCVgjQbWw4P"
      },
      "source": [
        "# calculate TF-IDF (in fact, multiply TF by IDF)\n",
        "tf_idf_vector = tfidf_trans.transform(bow_cv)\n",
        "tf_idf_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ0XAN5KXC94"
      },
      "source": [
        "# Now we can look at the TF-IDF score for a specific word in a specific document\n",
        "\n",
        "# to do this, we will convert the csr matrix into a regular Numpy array\n",
        "df_tfidf = pd.DataFrame(tf_idf_vector.toarray(), columns = vectorizer.get_feature_names_out())\n",
        "\n",
        "# and transpose it (write the columns as rows)\n",
        "print(df_tfidf.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZrpXBjrvC9h"
      },
      "source": [
        "# let's see how many words this method has left after processing\n",
        "df_tfidf.T.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv0WfZ87fjEp"
      },
      "source": [
        "#### Method 2. TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H6YPRJqzVtA"
      },
      "source": [
        "# importing the class TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEMHdtT738h_"
      },
      "source": [
        "# creating an object of the TfidfVectorizer class\n",
        "tfIdfVectorizer = TfidfVectorizer(use_idf = True, stop_words= 'english')\n",
        "\n",
        "# we immediately calculate the TF-IDF of words\n",
        "tfIdf = tfIdfVectorizer.fit_transform(sentences)\n",
        "tfIdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TAi6sbS9_Bn"
      },
      "source": [
        "# you can see which words are left after filtering\n",
        "print(tfIdfVectorizer.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwlWpb2HD6MI"
      },
      "source": [
        "# You can also view the IDF of the words\n",
        "tfIdfVectorizer.idf_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdyuC_xvFkNW"
      },
      "source": [
        "# through a dataframe, we can link words and their IDF\n",
        "\n",
        "df_idf = pd.DataFrame(tfIdfVectorizer.idf_, index = tfIdfVectorizer.get_feature_names_out(), columns = ['idf_weights'])\n",
        "# df_idf.sort_values(by = 'idf_weights', ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqDnLtnDA83v"
      },
      "source": [
        "# number of sentences (documents) x number of words\n",
        "tfIdf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjsM-1rrj3ro"
      },
      "source": [
        "Calculating the TF-IDF value for each word in each text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A47DW6LcAd4n"
      },
      "source": [
        "# and finally, the very meaning of TF-IDF for a specific word in a specific document\n",
        "# the more unique it is for a particular document, the higher the indicator\n",
        "df_tfidf = pd.DataFrame(tfIdf.toarray(), columns = tfIdfVectorizer.get_feature_names_out())\n",
        "print(df_tfidf.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbD1jsJ4epdk"
      },
      "source": [
        "Calculating the average TF-IDF value for each word across all texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbRZhvrreGP8"
      },
      "source": [
        "# calculate the arithmetic mean of the rows (axis = 0)\n",
        "tfIdf.mean(axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q8tWmeqdqnZ"
      },
      "source": [
        "# converting the matrix to a Numpy array\n",
        "np.asarray(tfIdf.mean(axis = 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg4bzPltfK4o"
      },
      "source": [
        "# see how many dimensions there are\n",
        "np.asarray(tfIdf.mean(axis = 0)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt5SB4N1d2Oy"
      },
      "source": [
        "# remove the second dimension\n",
        "np.asarray(tfIdf.mean(axis = 0)).ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2rUNoQ_fWk5"
      },
      "source": [
        "# we look at the dimension again\n",
        "np.asarray(tfIdf.mean(axis = 0)).ravel().shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVL8dWLjKsD1"
      },
      "source": [
        "# convert it to a list\n",
        "mean_weights = np.asarray(tfIdf.mean(axis = 0)).ravel().tolist()\n",
        "mean_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzUBcddZKvqU"
      },
      "source": [
        "# creating a dataframe from the dictionary\n",
        "mean_weights_df = pd.DataFrame({'term': tfIdfVectorizer.get_feature_names_out(), 'mean_weights': mean_weights})\n",
        "\n",
        "# we sort 10 words in descending order with the maximum average TF-IDF\n",
        "mean_weights_df.sort_values(by = 'mean_weights', ascending = False).reset_index(drop = True).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cosine distance between text vectors\n"
      ],
      "metadata": {
        "id": "2lehSjWkBG7j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjAmyNKgL6l0"
      },
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from scipy.sparse.csr import csr_matrix\n",
        "# import numpy as np\n",
        "# import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQXhKJhNMx2J"
      },
      "source": [
        "# let's take two texts (sentences) for simplicity\n",
        "text1 = 'all the world’s a stage, and all the men and women merely players'\n",
        "text2 = 'you must be the change you wish to see in the world'\n",
        "\n",
        "# combine them into a case\n",
        "corpus = [text1, text2]\n",
        "\n",
        "# creating an object of the TfidfVectorizer class\n",
        "tfIdfVectorizer = TfidfVectorizer(use_idf = True, stop_words = 'english')\n",
        "\n",
        "# at the output, we get two vectors, where each value is the weight (tf-idf indicator) of the word\n",
        "X = tfIdfVectorizer.fit_transform(corpus)\n",
        "\n",
        "# converting the data to a Numpy array\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTwMv_ZbO2cQ"
      },
      "source": [
        "# for convenience, we can look at the weights in the dataframe format\n",
        "vectors_df = pd.DataFrame(data = X.toarray(),\n",
        "                          index = ['vector1', 'vector2'],\n",
        "                          columns = tfIdfVectorizer.get_feature_names_out())\n",
        "vectors_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBDrj4OuMm8l"
      },
      "source": [
        "Let me remind you of the cosine distance formula\n",
        ":\n",
        "\n",
        "$$ \\cos \\theta ={\\mathbf {a} \\cdot \\mathbf {b} \\over \\|\\mathbf {a} \\|\\|\\mathbf {b} \\|} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2II-ulWhMqcU"
      },
      "source": [
        "# take the vectors separately\n",
        "vector1 = X.toarray()[0]\n",
        "vector2 = X.toarray()[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGSk7TsDTQ66"
      },
      "source": [
        "# first, we perform the operations in the numerator of the formula\n",
        "numerator = np.dot(vector1, vector2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYy73KmiTZNo"
      },
      "source": [
        "# now let's take the denominator and\n",
        "# (1) calculate the lengths (by and large, this is the Pythagorean theorem)\n",
        "vector1Len = np.linalg.norm(vector1)\n",
        "vector2Len = np.linalg.norm(vector2)\n",
        "\n",
        "# (2) multiply them\n",
        "denominator = vector1Len * vector2Len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZLhzIlQTo8D"
      },
      "source": [
        "# see what the cosine of the angle between the vectors is\n",
        "cosine = numerator/denominator\n",
        "cosine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GzHUtfMTqB9"
      },
      "source": [
        "# find the angle in degrees by its cosine\n",
        "# to do this, first calculate the angle in radians\n",
        "angle_radians = np.arccos(cosine)\n",
        "\n",
        "# then in degrees\n",
        "angle_degrees = angle_radians * 360/2/np.pi\n",
        "round(angle_degrees, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWV7Qcrc-R3K"
      },
      "source": [
        "#### Cluster text analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOxXBC9R-T5s"
      },
      "source": [
        "# There are two topics in the text below: data science and the Bolshoi Theater (source: Wikipedia)\n",
        "text = '''\n",
        "Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data.\n",
        "It applies knowledge and actionable insights from data across a broad range of application domains.\n",
        "Data science is related to data mining, machine learning and big data.\n",
        "The Bolshoi Theatre is a historic theatre in Moscow, Russia.\n",
        "It was originally designed by architect Joseph Bové, which holds ballet and opera performances.\n",
        "Before the October Revolution it was a part of the Imperial Theatres of the Russian Empire along with Maly Theatre in Moscow and a few theatres in Saint Petersburg.\n",
        "Data science is a concept to unify statistics, data analysis, informatics, and their related methods in order to understand and analyze actual phenomena with data.\n",
        "However, data science is different from computer science and information science.\n",
        "The main building of the theatre, rebuilt and renovated several times during its history, is a landmark of Moscow and Russia.\n",
        "On 28 October 2011, the Bolshoi re-opened after an extensive six-year renovation.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URq4UO70_HIe"
      },
      "source": [
        "# creating a list of suggestions\n",
        "corpus = []\n",
        "\n",
        "# to do this, in the for loop, we will go through the text, dividing it by the newline \\n\n",
        "for line in text.split('\\n'):\n",
        "\n",
        "  # if the string is not empty (i.e. True)\n",
        "  if line:\n",
        "\n",
        "    # we translate it to lowercase\n",
        "    line = line.lower()\n",
        "    # and add it to the list\n",
        "    corpus.append(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rykJ5ZyeBa1a"
      },
      "source": [
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMYN-CuWBgoY"
      },
      "source": [
        "# TfidfVectorizer example\n",
        "tfIdfVectorizer = TfidfVectorizer(use_idf = True, stop_words= 'english')\n",
        "\n",
        "# at the output, we get sentence vectors\n",
        "X = tfIdfVectorizer.fit_transform(corpus)\n",
        "# print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrfvoegnCjIq"
      },
      "source": [
        "# importing the k-means algorithm from the sklearn library\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# since we know that there are two topics, we use the hyperparameter k = 2\n",
        "kmeans = KMeans(n_clusters = 2, n_init = 10, random_state = 42).fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVBrBMMNC1I5"
      },
      "source": [
        "# take the new proposals, one from the field of Data Science and two about the Bolshoi Theater\n",
        "prediction = ['Many statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics.',\n",
        "              'Urusov set up the theatre in collaboration with English tightrope walker Michael Maddox.',\n",
        "              'Until the mid-1990s, most foreign operas were sung in Russian, but Italian and other languages have been heard more frequently on the Bolshoi stage in recent years.']\n",
        "\n",
        "# let's apply two models, first we will create vectors of new sentences (TfidfVectorizer.transform),\n",
        "# then we assign them to one of the clusters (kmeans.predict)\n",
        "kmeans.predict(tfIdfVectorizer.transform(prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCDTXKGkaVZ-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}